name: Complete CI/CD Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  DOCKER_IMAGE: azexkush/car-lot-manager
  DOCKER_TAG: latest

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and Push Docker Image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
            ${{ env.DOCKER_IMAGE }}:${{ github.sha }}
          cache-from: type=registry,ref=${{ env.DOCKER_IMAGE }}:buildcache
          cache-to: type=registry,ref=${{ env.DOCKER_IMAGE }}:buildcache,mode=max

      - name: Verify Image
        run: |
          docker pull ${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
          docker run --rm ${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }} python -c "import flask; print('Flask version:', flask.__version__)"

  deploy-infrastructure:
    name: Deploy Infrastructure and Application
    needs: build-and-push
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_wrapper: false

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Destroy Old Resources
        working-directory: ./terraform
        run: terraform destroy -auto-approve
        continue-on-error: true

      - name: Terraform Apply
        working-directory: ./terraform
        run: terraform apply -auto-approve

      - name: Get Infrastructure Details
        id: infra
        working-directory: ./terraform
        run: |
          ALB_DNS=$(terraform output -raw alb_dns_name)
          MASTER_PRIVATE_IP=$(terraform output -raw master_private_ip)
          INSTANCE_IPS=$(terraform output -json instance_ips)
          
          MASTER_IP=$(echo $INSTANCE_IPS | jq -r '.[0]')
          WORKER1_IP=$(echo $INSTANCE_IPS | jq -r '.[1]')
          WORKER2_IP=$(echo $INSTANCE_IPS | jq -r '.[2]')
          
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
          echo "master_ip=$MASTER_IP" >> $GITHUB_OUTPUT
          echo "master_private_ip=$MASTER_PRIVATE_IP" >> $GITHUB_OUTPUT
          echo "worker1_ip=$WORKER1_IP" >> $GITHUB_OUTPUT
          echo "worker2_ip=$WORKER2_IP" >> $GITHUB_OUTPUT
          
          echo "Infrastructure Created:"
          echo "Master: $MASTER_IP (Private: $MASTER_PRIVATE_IP)"
          echo "Worker 1: $WORKER1_IP"
          echo "Worker 2: $WORKER2_IP"
          echo "Load Balancer: $ALB_DNS"
          
          terraform output -raw ssh_private_key > /tmp/ssh_key.pem
          chmod 400 /tmp/ssh_key.pem

      - name: Wait for Instances to Boot
        run: sleep 90

      - name: Test SSH Connectivity
        run: |
          for ip in "${{ steps.infra.outputs.master_ip }}" "${{ steps.infra.outputs.worker1_ip }}" "${{ steps.infra.outputs.worker2_ip }}"; do
            echo "Testing SSH to $ip..."
            max_attempts=30
            attempt=1
            
            while [ $attempt -le $max_attempts ]; do
              if ssh -i /tmp/ssh_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=5 ubuntu@$ip "echo OK" 2>/dev/null; then
                echo "âœ“ SSH ready on $ip"
                break
              fi
              
              if [ $attempt -eq $max_attempts ]; then
                echo "âœ— Failed to connect to $ip"
                exit 1
              fi
              
              sleep 10
              attempt=$((attempt + 1))
            done
          done

      - name: Create Ansible Inventory
        run: |
          cat > ansible/inventory.ini << EOF
          [master]
          ${{ steps.infra.outputs.master_ip }} ansible_user=ubuntu ansible_ssh_private_key_file=/tmp/ssh_key.pem ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          
          [worker]
          ${{ steps.infra.outputs.worker1_ip }} ansible_user=ubuntu ansible_ssh_private_key_file=/tmp/ssh_key.pem ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          ${{ steps.infra.outputs.worker2_ip }} ansible_user=ubuntu ansible_ssh_private_key_file=/tmp/ssh_key.pem ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          
          [all:vars]
          ansible_python_interpreter=/usr/bin/python3
          EOF
          
          cat ansible/inventory.ini

      - name: Install Ansible
        run: sudo apt-get update && sudo apt-get install -y ansible

      - name: Run Ansible Playbook
        run: |
          cd ansible
          ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory.ini playbook.yml -vv

      - name: Wait for Kubernetes Cluster
        run: sleep 60

      - name: Deploy Application
        run: |
          echo "Deploying application to Kubernetes..."
          
          # Copy deployment manifest
          scp -i /tmp/ssh_key.pem -o StrictHostKeyChecking=no ./simple-deployment.yaml ubuntu@${{ steps.infra.outputs.master_ip }}:/tmp/
          
          # Deploy application
          ssh -i /tmp/ssh_key.pem -o StrictHostKeyChecking=no ubuntu@${{ steps.infra.outputs.master_ip }} "
            # Wait for nodes to be ready
            kubectl wait --for=condition=Ready nodes --all --timeout=300s
            
            # Apply deployment
            kubectl apply -f /tmp/simple-deployment.yaml
            
            # Wait for deployment to be ready
            kubectl rollout status deployment/car-lot-simple --timeout=300s
            
            # Show status
            echo 'Pods:'
            kubectl get pods -o wide
            echo 'Services:'
            kubectl get svc
            echo 'Nodes:'
            kubectl get nodes
          "

      - name: Verify NodePort
        run: |
          echo "Testing NodePort on all nodes..."
          
          for ip in "${{ steps.infra.outputs.master_ip }}" "${{ steps.infra.outputs.worker1_ip }}" "${{ steps.infra.outputs.worker2_ip }}"; do
            echo "Testing $ip:30080..."
            ssh -i /tmp/ssh_key.pem -o StrictHostKeyChecking=no ubuntu@${{ steps.infra.outputs.master_ip }} "
              curl -s --connect-timeout 5 http://$ip:30080/health || echo 'Failed'
            "
          done

      - name: Wait for Load Balancer
        run: |
          echo "Waiting for Load Balancer to register targets..."
          sleep 120

      - name: Test Load Balancer
        run: |
          APP_URL="http://${{ steps.infra.outputs.alb_dns }}"
          
          echo "Testing Load Balancer at $APP_URL"
          
          max_attempts=20
          attempt=1
          success=false
          
          while [ $attempt -le $max_attempts ]; do
            echo "Attempt $attempt/$max_attempts..."
            
            if curl -s --connect-timeout 10 "$APP_URL/health" | grep -q "healthy"; then
              echo "âœ… Health check passed!"
              
              if curl -s --connect-timeout 10 "$APP_URL" | grep -q "Car Lot Manager"; then
                echo "âœ… Application is fully accessible!"
                success=true
                break
              fi
            fi
            
            if [ $attempt -eq $max_attempts ]; then
              echo "âŒ Application not accessible after $max_attempts attempts"
              
              # Debug information
              echo "Checking target health..."
              TG_ARN=$(aws elbv2 describe-target-groups --query "TargetGroups[?contains(TargetGroupName, 'carlot-tg')].TargetGroupArn" --output text | head -1)
              if [ -n "$TG_ARN" ]; then
                aws elbv2 describe-target-health --target-group-arn $TG_ARN
              fi
              
              exit 1
            fi
            
            sleep 15
            attempt=$((attempt + 1))
          done

      - name: Final Deployment Summary
        run: |
          echo ""
          echo "=========================================="
          echo "ðŸŽ‰ DEPLOYMENT COMPLETED SUCCESSFULLY! ðŸŽ‰"
          echo "=========================================="
          echo ""
          echo "ðŸŒ Access your Car Lot Manager at:"
          echo ""
          echo "    http://${{ steps.infra.outputs.alb_dns }}"
          echo ""
          echo "=========================================="
          echo ""
          echo "Infrastructure Details:"
          echo "  - Master Node: ${{ steps.infra.outputs.master_ip }}"
          echo "  - Worker 1: ${{ steps.infra.outputs.worker1_ip }}"
          echo "  - Worker 2: ${{ steps.infra.outputs.worker2_ip }}"
          echo "  - Load Balancer: ${{ steps.infra.outputs.alb_dns }}"
          echo ""
          echo "Application Endpoints:"
          echo "  - Main Page: http://${{ steps.infra.outputs.alb_dns }}/"
          echo "  - Health Check: http://${{ steps.infra.outputs.alb_dns }}/health"
          echo "  - API: http://${{ steps.infra.outputs.alb_dns }}/api/inventory"
          echo ""
          echo "=========================================="
          
          echo "http://${{ steps.infra.outputs.alb_dns }}" > deployment_url.txt

      - name: Upload Deployment URL
        uses: actions/upload-artifact@v4
        with:
          name: deployment-url
          path: deployment_url.txt
